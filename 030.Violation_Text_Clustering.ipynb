{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Clustering\n",
    "In this document, I will cluster text data for violation from across many cities in the state to see if it creates a reasonable categories to use for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "\n",
    "import spacy\n",
    "import en_core_web_lg\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "pd.set_option('precision', 4)\n",
    "pd.options.display.max_seq_items = 100\n",
    "pd.options.display.max_columns = 50\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('DATA/text.txt', 'rb') as filepath:\n",
    "    text_list = pickle.load(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75996170"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a huge dataset, we should be careful dealing with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(text_list, columns=['violations'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"([a-zA-Z]+[\\&[a-zA-Z]+]?)\"\n",
    "p = re.compile(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['violations'] = df.violations.map(lambda x: ' '.join(p.findall(str(x).upper())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates\n",
    "df.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(439438, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's substantially less, so that's great! Let's save them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DATA/text_clean.txt\", \"wb\") as filepath:\n",
    "    pickle.dump(df.violations.values, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = [x.lower() for x in list(df.violations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_words(textlist, n = 1, thresh = .8, nlp = None, replace = False, manual_spellcheck = False):\n",
    "    '''\n",
    "    Takes a list of texts to run iteration\n",
    "    Change word that occurs only once to similar word in the text\n",
    "    Return consolidated list \n",
    "    '''\n",
    "    if nlp == None:\n",
    "        nlp = en_core_web_lg.load()\n",
    "    \n",
    "    list_of_text = textlist.copy()\n",
    "    \n",
    "    wordcounts = Counter(' '.join(list_of_text).split())    \n",
    "    words_once = [k for k, v in wordcounts.items() if v <= n]\n",
    "    other_words = [k for k, v in wordcounts.items() if v > n]\n",
    "    \n",
    "    tokens = nlp(' '.join(other_words))\n",
    "    replacement_dict = {}\n",
    "    \n",
    "    for word in words_once:\n",
    "\n",
    "        word_token = nlp(word)\n",
    "        max_similarity = thresh\n",
    "\n",
    "        for tk in tokens:\n",
    "            # find the maximum similarity above threshold\n",
    "            sim_score = word_token.text, tk.text, word_token.similarity(tk)\n",
    "            if 1 > sim_score[2] > max_similarity:\n",
    "                replacement_dict[word] = sim_score[1]\n",
    "                max_similarity = sim_score[2]\n",
    "        try:\n",
    "            print(word, 'to', replacement_dict[word])\n",
    "        except KeyError:\n",
    "            if manual_spellcheck:\n",
    "                ans = input(f'{word} does not have a replacement. If you have a suggesion, type the word, otherwise press spacebar')\n",
    "                if ans == ' ':\n",
    "                    continue\n",
    "                else:\n",
    "                    replacement_dict[word] = ans\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "    if replace:\n",
    "        for i, text in enumerate(list_of_text):\n",
    "\n",
    "            text = text.split()\n",
    "            for j, te in enumerate(text):\n",
    "                if te in replacement_dict: \n",
    "                    text[j] = replacement_dict[te]\n",
    "            list_of_text[i] = ' '.join(text)\n",
    "\n",
    "        return list_of_text\n",
    "    \n",
    "    else:\n",
    "        return replacement_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paying to pay\n",
      "endanger to endangering\n",
      "focused to focus\n",
      "dogs to dog\n",
      "pattern to patterns\n",
      "flanges to flange\n",
      "districts to district\n",
      "thirty to fifty\n",
      "prevents to preventing\n",
      "sleds to sled\n",
      "growing to grow\n",
      "horses to horse\n",
      "deceit to deception\n",
      "automobiles to automobile\n",
      "charges to charged\n",
      "wanted to did\n",
      "drink to drinking\n",
      "prohibits to prohibiting\n",
      "comme to fait\n",
      "dallas to houston\n",
      "finishing to finished\n",
      "awards to award\n",
      "strobes to strobe\n",
      "axles to axle\n",
      "slowed to slowing\n",
      "staying to stay\n",
      "wagering to gambling\n",
      "wyoming to idaho\n",
      "tennessee to arkansas\n",
      "gold to silver\n",
      "boston to chicago\n",
      "forget to remember\n",
      "they to have\n",
      "lists to list\n",
      "couldn to didn\n",
      "chips to chip\n",
      "arrested to arrest\n",
      "garbage to trash\n",
      "few to several\n",
      "even to but\n",
      "husband to wife\n",
      "promises to promise\n",
      "ever to never\n",
      "probs to prob\n",
      "brothers to brother\n",
      "interior to exterior\n",
      "early to late\n",
      "depressants to depressant\n",
      "responsibilty to responsiblity\n",
      "exessive to excesive\n",
      "though to but\n",
      "mutliple to mutiple\n",
      "trains to train\n",
      "horizontally to vertically\n",
      "ive to ve\n",
      "cotrol to conrol\n",
      "zolpidem to alprazolam\n",
      "islands to island\n",
      "qualifications to qualification\n",
      "locking to lock\n",
      "assemblies to assembly\n",
      "times] to ]\n",
      "securty to secuirty\n",
      "continuos to continous\n",
      "conditons to condtions\n",
      "got to getting\n",
      "there to but\n",
      "babies to baby\n",
      "burned to burnt\n",
      "neither to nor\n",
      "safty to saftey\n",
      "fled to fleeing\n",
      "northbound to eastbound\n",
      "thought to did\n",
      "eat to eating\n",
      "sets to set\n",
      "applications to application\n",
      "whisky to whiskey\n",
      "diazepam to alprazolam\n",
      "talk to talking\n",
      "motors to motor\n",
      "mississippi to alabama\n",
      "copies to copy\n",
      "aproach to approch\n",
      "calif to bakersfield\n",
      "maintance to maintence\n",
      "modesto to bakersfield\n",
      "labels to label\n",
      "duplicates to duplicate\n",
      "jacksonville to tulsa\n",
      "detroit to chicago\n",
      "lubbock to bakersfield\n",
      "began to started\n",
      "leaks to leak\n",
      "officials to authorities\n",
      "stainless to steel\n",
      "grandson to son\n",
      "michael to jackson\n",
      "took to went\n",
      "intentional to deliberate\n",
      "simulating to simulate\n",
      "utah to idaho\n",
      "pill to pills\n",
      "groups to group\n",
      "warnings to warning\n",
      "think to know\n",
      "exempted to exempt\n",
      "rentals to rental\n",
      "what to how\n",
      "pleae to plese\n",
      "presciption to perscription\n",
      "casino to gambling\n",
      "wasnt to nt\n",
      "harrasment to harrassment\n",
      "catching to catch\n",
      "playing to play\n",
      "exemptions to exemption\n",
      "requested to request\n",
      "once to again\n",
      "waiting to wait\n",
      "untill to till\n",
      "mantain to maintian\n",
      "tramadol to hydrocodone\n",
      "traveld to travled\n",
      "dimensions to dimension\n",
      "camaro to mustang\n",
      "assult to assualt\n",
      "ammunition to ammo\n",
      "committing to commit\n",
      "circumstance to circumstances\n",
      "oxycodone to hydrocodone\n",
      "displays to display\n",
      "taxes to tax\n",
      "evansville to topeka\n",
      "remark to remarks\n",
      "seattle to washington\n",
      "carrollton to clarksville\n",
      "oxnard to bakersfield\n",
      "portland to oregon\n",
      "indianapolis to indiana\n",
      "containers to container\n",
      "waco to tulsa\n",
      "moose to deer\n",
      "louisville to kentucky\n",
      "aggrivated to aggrevated\n",
      "faster to slower\n",
      "adjacent to adjoining\n",
      "repairs to repair\n",
      "skateboarding to skateboard\n",
      "emblems to emblem\n",
      "manufacturers to manufacturer\n",
      "identities to identity\n"
     ]
    }
   ],
   "source": [
    "new_tmp = consolidate_words(tmp, nlp = nlp, replace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "require to requires\n",
      "arizona to colorado\n",
      "refusing to refused\n",
      "higher to lower\n",
      "bars to bar\n",
      "small to large\n",
      "keeping to keep\n",
      "paths to path\n",
      "lots to lot\n",
      "fees to fee\n",
      "attend to attended\n",
      "flags to flag\n",
      "provision to provisions\n",
      "stations to station\n",
      "exceptions to exception\n",
      "fight to fighting\n",
      "motorbike to motorcycle\n",
      "scooters to scooter\n",
      "parks to park\n",
      "milage to mileage\n",
      "connection to connections\n",
      "skateboard to skateboards\n",
      "placards to placard\n",
      "sidewalks to sidewalk\n",
      "crime to crimes\n",
      "indicated to indicate\n",
      "responsibilities to responsibility\n",
      "signaling to signalling\n",
      "rent to rental\n",
      "permanent to temporary\n",
      "sending to send\n",
      "gates to gate\n",
      "then to when\n",
      "exercising to exercise\n",
      "riders to rider\n",
      "routes to route\n",
      "introducing to introduce\n",
      "misdemeanors to felonies\n",
      "so to too\n",
      "robbery to burglary\n",
      "felonies to misdemeanors\n",
      "intention to intent\n",
      "seats to seat\n",
      "buying to purchasing\n",
      "causes to cause\n",
      "approval to approved\n",
      "lbs] to lbs\n",
      "controls to control\n",
      "plants to plant\n",
      "paid to pay\n",
      "chemical to chemicals\n",
      "orange to yellow\n",
      "travelling to traveling\n",
      "will to be\n",
      "winds to wind\n",
      "came to had\n",
      "ass to butt\n",
      "nothing to but\n",
      "names to name\n",
      "obsolete to outdated\n",
      "trail to trails\n",
      "cat to dog\n",
      "prohibiting to prohibit\n",
      "oversized to oversize\n",
      "fact to that\n",
      "lands to land\n",
      "purple to blue\n",
      "penalties to penalty\n",
      "oklahoma to missouri\n",
      "pistol to rifle\n",
      "houston to tx\n",
      "colors to color\n",
      "putting to put\n",
      "weigh to weighing\n",
      "cones to cone\n",
      "firearms to firearm\n",
      "east to west\n",
      "limiting to restricting\n",
      "locks to lock\n",
      "employees to employee\n",
      "else to if\n",
      "wisconsin to minnesota\n",
      "michigan to minnesota\n",
      "ago to months\n",
      "daughter to son\n",
      "kansas to missouri\n",
      "follwoing to folowing\n",
      "idaho to colorado\n",
      "arkansas to missouri\n",
      "oregon to iowa\n",
      "ohio to illinois\n",
      "hydrocodone to codeine\n",
      "just to it\n",
      "he to him\n",
      "forms to form\n",
      "yet to but\n",
      "addresses to address\n",
      "better to good\n",
      "you to sure\n",
      "chicago to illinois\n",
      "could to should\n",
      "very to too\n",
      "tell to know\n",
      "allows to allow\n",
      "detectors to detector\n",
      "shows to show\n",
      "losing to lost\n",
      "kids to children\n",
      "ear to ears\n",
      "going to go\n",
      "denver to colorado\n",
      "expires to expire\n",
      "several to many\n",
      "been to being\n",
      "girls to girl\n",
      "toyota to nissan\n",
      "would to should\n",
      "already to still\n",
      "wiper to wipers\n",
      "incorrectly to correctly\n",
      "wires to wire\n",
      "payments to payment\n",
      "alabama to kentucky\n",
      "switching to switch\n",
      "markers to marker\n",
      "pontiac to buick\n",
      "chevy to ford\n",
      "york to ny\n",
      "buses to bus\n",
      "pipes to pipe\n",
      "flap to flaps\n",
      "withdrawl to withdrawls\n",
      "communications to communication\n",
      "carriers to carrier\n",
      "acual to actaul\n",
      "traps to trap\n",
      "methods to method\n",
      "follwing to folowing\n",
      "getting to get\n",
      "perscription to prescription\n",
      "clocks to clock\n",
      "fifth to fourth\n",
      "inspections to inspection\n",
      "straps to strap\n",
      "seven to five\n",
      "posess to posses\n",
      "cannot to can\n",
      "decals to decal\n",
      "pursue to pursuing\n",
      "actually to still\n",
      "rx to prescription\n",
      "brackets to bracket\n",
      "fraudlent to fradulent\n",
      "cant to nt\n",
      "dont to nt\n",
      "february to january\n",
      "violent to violence\n",
      "wore to wearing\n",
      "injuries to injury\n",
      "south to west\n",
      "medications to drugs\n",
      "dakota to nebraska\n",
      "maintaining to maintain\n",
      "numerous to many\n",
      "great to good\n",
      "services to service\n",
      "books to book\n",
      "sections to section\n",
      "dividers to divider\n",
      "north to west\n",
      "carolina to nc\n",
      "georgia to nc\n",
      "bottles to bottle\n",
      "won to win\n",
      "six to five\n",
      "nine to five\n",
      "always to never\n",
      "works to work\n",
      "picture to photo\n",
      "acquiring to acquire\n",
      "bikes to bike\n",
      "exceeds to exceed\n",
      "virginia to kentucky\n",
      "didnt to nt\n",
      "charger to battery\n",
      "blocks to block\n",
      "parties to party\n",
      "headset to headsets\n",
      "objects to object\n",
      "benefits to benefit\n",
      "contain to contains\n",
      "amphetamines to amphetamine\n",
      "tests to test\n"
     ]
    }
   ],
   "source": [
    "# running one more\n",
    "new_tmp2 = consolidate_words(new_tmp, n = 10, nlp = nlp, replace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['violations'] = new_tmp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DATA/text_clean.txt\", \"wb\") as filepath:\n",
    "    pickle.dump(df.violations.values, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['violations'] = df.violations.apply(lambda x: [wnl.lemmatize(word) for word in x.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-60592365fa36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DATA/text_clean.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "#with open(\"DATA/text_clean.txt\", \"rb\") as filepath:\n",
    "#    df.violations = pickle.load(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique words\n",
    "Let's get a set of unique words and make some final edits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = set()\n",
    "\n",
    "for text in df.violations:\n",
    "    unique = unique | set(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-1f550409a529>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# get word counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviolations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2181\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0;32m-> 2182\u001b[0;31m                           initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# get word counts\n",
    "counts = Counter(np.sum(df.violations.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More corrections\n",
    "It seems like some words don't have proper spacing between them I'll try to add them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'words-by-frequency.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-ebd875c60484>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Build a cost dictionary, assuming Zipf's law and cost = -math.log(probability).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"words-by-frequency.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mwordcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmaxword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'words-by-frequency.txt'"
     ]
    }
   ],
   "source": [
    "# Credit for below code goes to Generic Human \n",
    "#(https://stackoverflow.com/questions/8870261/how-to-split-text-without-spaces-into-list-of-words) \n",
    "\n",
    "from math import log\n",
    "\n",
    "# Build a cost dictionary, assuming Zipf's law and cost = -math.log(probability).\n",
    "words = counts\n",
    "wordcost = dict((k, log((i+1)*log(len(words)))) for i,k in enumerate(words))\n",
    "maxword = max(len(x) for x in words)\n",
    "\n",
    "def infer_spaces(s):\n",
    "    \"\"\"Uses dynamic programming to infer the location of spaces in a string\n",
    "    without spaces.\"\"\"\n",
    "\n",
    "    # Find the best match for the i first characters, assuming cost has\n",
    "    # been built for the i-1 first characters.\n",
    "    # Returns a pair (match_cost, match_length).\n",
    "    def best_match(i):\n",
    "        candidates = enumerate(reversed(cost[max(0, i-maxword):i]))\n",
    "        return min((c + wordcost.get(s[i-k-1:i], 9e999), k+1) for k,c in candidates)\n",
    "\n",
    "    # Build the cost array.\n",
    "    cost = [0]\n",
    "    for i in range(1,len(s)+1):\n",
    "        c,k = best_match(i)\n",
    "        cost.append(c)\n",
    "\n",
    "    # Backtrack to recover the minimal-cost string.\n",
    "    out = []\n",
    "    i = len(s)\n",
    "    while i>0:\n",
    "        c,k = best_match(i)\n",
    "        assert c == cost[i]\n",
    "        out.append(s[i-k:i])\n",
    "        i -= k\n",
    "\n",
    "    return \" \".join(reversed(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "tm = nlp('warningdrivingtooslowinleftlane')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "First, I'll loop through all unique words and try to converge some of the similar ones. That will correct some typos.\n",
    "Then I can look at clustering in two different ways\n",
    "1. see individual word clustering\n",
    "2. see the average coordinate clustering (average point of all words in each observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Word Embeddings\n",
    "Now I'll get the word embeddings for each words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
